{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "a9aca04a-0bc2-4199-bf6c-090ed3e7a1a2",
   "metadata": {},
   "source": [
    "### 000 TOP <a id=\"title_page\"></a>\n",
    "\n",
    "#. Granger Causality:\n",
    "#What It Does: Tests whether past values of one time series (X) help predict another time series (Y), given past values of Y.\n",
    "#Assumptions:\n",
    "#Assumes a linear relationship between variables.\n",
    "#Requires stationarity of time series dat\n",
    "\n",
    "#Machine earnins\n",
    "#No explicit assumption of linearity (can capture nonlinear relationships).\n",
    "#Random Forests: A collection of decision trees used for regression or classification tasks.\n",
    "#XGBoost: A boosting algorithm that builds strong models iteratively, reducing errors step by step.\n",
    "#Permutation Importance: Evaluates the importance of features by shuffling them and measuring the drop in model performance.\n",
    "\n",
    "make funcitons the grpahs\n",
    "make the data indifferneces. \n",
    "funciton s add th name of the varialbe int he grpahs at the begigngins, \n",
    "add a watermanrket of the varaibles.  int eh time series plots. \n",
    "\n",
    "\n",
    "reduce size of letter\n",
    "make data ftransfroamtions int he fetaure engienering \n",
    "make data tgransformatnion int eh fuciton graphs \n",
    "do another varauble y \n",
    "\n",
    "\n",
    "in the final verison ther eill be only onve veriosn hat does the algorith g\n",
    "and another veerison that jsuit read teh parquet and  updates tyhe graphs  \n",
    "add ast date to the aLGORTHSM IMAGE   \n",
    "\n",
    "\n",
    "\n",
    "\n",
    "### 000 packages <a id=\"title_page\"></a>\n",
    "- [Top](#table_contents)\n",
    "\n",
    "### random forest\n",
    "## Granger causality tests\n",
    "## Pair levy\n",
    "##leading laggin analysius\n",
    "## visual mpas\n",
    "\n",
    "\n",
    "from xgboost import XGBRegressor\n",
    "import lightgbm as lgb\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "owner=\"\"\n",
    "###econiomics DASHBOARD\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "from IPython.display import display, HTML\n",
    "import Haver\n",
    "Haver.path(Haver.direct(1))    \n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.decomposition import PCA\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib as mpl\n",
    "%matplotlib inline\n",
    "import os\n",
    "def clear():\n",
    "    os.system('cls')\n",
    "# then you can clear the window with:\n",
    "from matplotlib.dates import date2num\n",
    "import os\n",
    "os.chdir('C:/database/dashboard')\n",
    "#C:\\database\\dashboard\n",
    "print(os.getcwd())\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.collections import LineCollection\n",
    "import matplotlib.ticker as plticker\n",
    "import matplotlib.dates as mdates\n",
    "from matplotlib.ticker import MultipleLocator\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as pit\n",
    "from PIL import Image\n",
    "import seaborn as sns\n",
    "import plotly.graph_objects as go\n",
    "from matplotlib.colors import LinearSegmentedColormap\n",
    "import matplotlib.pyplot as pit\n",
    "from PIL import Image\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn import svm, metrics\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.metrics import mean_squared_error, r2_score\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.inspection import permutation_importance\n",
    "from statsmodels.tsa.stattools import grangercausalitytests\n",
    "from statsmodels.tsa.stattools import grangercausalitytests\n",
    "from statsmodels.tsa.api import adfuller\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")  # Suppress all warnings\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import r2_score\n",
    "from statsmodels.tsa.stattools import adfuller\n",
    "import statsmodels.api as sm\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import r2_score\n",
    "from statsmodels.tsa.stattools import adfuller\n",
    "import statsmodels.api as sm\n",
    "####Granger causality, ensure statiosnaruty of data. \n",
    "from statsmodels.tsa.stattools import grangercausalitytests\n",
    "from statsmodels.tsa.stattools import grangercausalitytests\n",
    "from statsmodels.tsa.api import adfuller\n",
    "from tqdm import tqdm\n",
    "\n",
    "\n",
    "\n",
    "#param_grid = {'n_estimators':range(1,51,5),'max_leaf_nodes': range(2,10)}\n",
    "##rf = RandomForestClassifier()\n",
    "#gs = GridSearchCV(rf,param_grid=param_grid)\n",
    "#rs = gs.fit(df_std3_train_X,df_std3_train_Y)\n",
    "\n",
    "### 000 parameters\n",
    "\n",
    "top_n = top_n0= 20\n",
    "random_state=random_state0=99\n",
    "size_widht=size_widht0=6\n",
    "size_height=size_height0=7\n",
    "rf_n_estimators=rf_n_estimators0=350\n",
    "per_n_repeats=per_n_repeats0=4\n",
    "xgb_n_estimators=xgb_n_estimators0=1000\n",
    "lgb_n_estimators=lgb_n_estimators0=4000\n",
    "text_title_s=11\n",
    "text_y=6\n",
    "ticks_vertical=5\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "### 000 feature_engineering function\n",
    "\n",
    "\n",
    "def feature_engineering(x_data_input, lags=[1, 2, ], random_state=999):\n",
    "    \"\"\"\n",
    "    Performs feature engineering on the dataset, including rolling averages, differences,\n",
    "    percentage changes, z-scores, and lag variables, with a progress bar for tracking execution.\n",
    "    \n",
    "    Parameters:\n",
    "        x_data_input (pd.DataFrame): Input DataFrame.\n",
    "        lags (list, optional): List of lag values to generate. Default is [1, 2, 3, 4].\n",
    "        random_state (int, optional): Random state for reproducibility. Default is 999.\n",
    "\n",
    "    Returns:\n",
    "        dict: Contains:\n",
    "            - \"x_featured_data\": DataFrame with feature-engineered variables.\n",
    "            - \"x_scaled_data\": DataFrame with missing values handled.\n",
    "    \"\"\"\n",
    "\n",
    "    # Copy dataset\n",
    "    x_data = x_data_input.copy()\n",
    "    print(x_data.index.min())\n",
    "    print(x_data.index.max())\n",
    "    print(x_data.shape[0])\n",
    "    print(x_data.shape[1])\n",
    "    print(x_data.columns.tolist())\n",
    "\n",
    "\n",
    "    # Clean column names\n",
    "    x_data.columns = x_data.columns.str.replace(r'[^\\w]', '_', regex=True)\n",
    "    x_data.columns = x_data.columns.str.replace(' ', '_', regex=True)\n",
    "\n",
    "    # Initialize progress bar\n",
    "    total_steps = 5  # Number of feature engineering processes\n",
    "    progress_bar = tqdm(total=total_steps, desc=\"Feature Engineering\", position=0, leave=True)\n",
    "\n",
    "    # ========== FEATURE ENGINEERING ==========\n",
    "    df = x_data.copy()\n",
    "\n",
    "\n",
    "    print(\"Rolling Averages start\")\n",
    "    # Rolling averages\n",
    "    columns_list = df.columns.tolist()\n",
    "    for col in columns_list:\n",
    "        df[f\"{col}_rolling_avg2\"] = df[col].rolling(window=2).mean()\n",
    "        df[f\"{col}_rolling_avg3\"] = df[col].rolling(window=3).mean()\n",
    "        df[f\"{col}_rolling_avg6\"] = df[col].rolling(window=6).mean()\n",
    "    progress_bar.update(1)  # Update progress\n",
    "    progress_bar.set_description(\"Rolling Averages Complete\")\n",
    "    print(\"Rolling Averages Complete\")\n",
    "\n",
    "    print(\"Difference Calculation start\")\n",
    "    # Differences\n",
    "    columns_list = df.columns.tolist()\n",
    "    for col in columns_list:\n",
    "        df[f\"{col}_diff1\"] = df[col].diff()\n",
    "    progress_bar.update(1)  # Update progress\n",
    "    progress_bar.set_description(\"Difference Calculation Complete\")\n",
    "    print(\"Difference Calculation Complete\")\n",
    "\n",
    "\n",
    "\n",
    "    print(\"Percentage Changes start\")\n",
    "    # Percentage changes\n",
    "    columns_list = df.columns.tolist()\n",
    "    for col in columns_list:\n",
    "        df[f\"{col}_fmt_yy\"] = df[col].pct_change(12).multiply(100).round(3)\n",
    "        df[f\"{col}_fmt_qq\"] = df[col].pct_change(4).multiply(100).round(3)\n",
    "        df[f\"{col}_fmt_mm\"] = df[col].pct_change(1).multiply(100).round(3)\n",
    "    progress_bar.update(1)  # Update progress\n",
    "    progress_bar.set_description(\"Percentage Changes Complete\")\n",
    "    print(\"Percentage Changes Complete\")\n",
    "\n",
    "\n",
    "    print(\"Z-score Calculation start\")\n",
    "    # Z-score calculations\n",
    "    columns_list = df.columns.tolist()\n",
    "    list_exc = [\"_diff1_fmt_yy\",\"_diff1_fmt_qq\",\"_diff1_fmt_mm\"]\n",
    "    columns_list = list(filter(lambda col: not any(exc in col for exc in list_exc), columns_list))\n",
    "\n",
    "\n",
    "    \n",
    "    for col in columns_list:\n",
    "        #for window in [12, 24, 36]:  # Moving average windows\n",
    "        for window in [ 24]:  # Moving average windows\n",
    "            df[f\"{col}_mavg{window}\"] = df[col].rolling(window=window).mean()\n",
    "            df[f\"{col}_std_dev{window}\"] = df[col].rolling(window=window).std()\n",
    "            df[f\"{col}_zscore{window}\"] = (df[col] - df[f\"{col}_mavg{window}\"]) / df[f\"{col}_std_dev{window}\"]\n",
    "    progress_bar.update(1)  # Update progress\n",
    "    progress_bar.set_description(\"Z-score Calculation Complete\")\n",
    "    print(\"Z-score Calculation Complete\")\n",
    "\n",
    "\n",
    "    print(\"Lag Variables start\")\n",
    "    # Lag variables\n",
    "    columns_list = df.columns.tolist()\n",
    "    for col in df.columns:\n",
    "        print(col)\n",
    "        for lag in lags:\n",
    "            df[f'{col}_lag{lag}'] = df[col].shift(lag)\n",
    "    progress_bar.update(1)  # Update progress\n",
    "    progress_bar.set_description(\"Lag Variables Complete\")\n",
    "    print(\"Lag Variables Completee\")\n",
    "    # Close progress bar\n",
    "    progress_bar.close()\n",
    "\n",
    "    # Replace infinite values and fill missing values\n",
    "    df.replace([np.inf, -np.inf], 0, inplace=True)\n",
    "    df.fillna(0, inplace=True)\n",
    "    x_scaled = df.copy()\n",
    "\n",
    "    print(\"Feature Engineering Complete \")\n",
    "\n",
    "    return {\n",
    "        \"x_featured_data\": df,\n",
    "        \"x_scaled_data\": x_scaled\n",
    "    }\n",
    "\n",
    "\n",
    "### 000 function preporcessing and splitting data\n",
    "\n",
    "def preprocess_and_split_data(x_dataframe, variable_y, original_y, date_beg=\"2015-01-01\",lags=[1, 2, 3],):\n",
    "\n",
    "    # Copy dataset\n",
    "    x_data = x_dataframe.copy()\n",
    "    x_data=x_data[x_data.index>=date_beg]\n",
    "    #print(\"Initial Data Overview:\")\n",
    "    #print(x_data.index.min())\n",
    "    #print(x_data.tail(5))\n",
    "    # Clean column names\n",
    "    x_data.columns = x_data.columns.str.replace(r'[^\\w]', '_', regex=True)\n",
    "    x_data.columns = x_data.columns.str.replace(' ', '_', regex=True)\n",
    "    #print(\"Processed Column Names:\", x_data.columns.tolist())\n",
    "\n",
    "    # Process Target Variable\n",
    "    y_data_input = x_data[[variable_y]].copy()\n",
    "    y_data_input = y_data_input.dropna()\n",
    "    y_data = y_data_input.copy()\n",
    "\n",
    "    #print(\"Target Variable Processed:\")\n",
    "    #print(y_data.index.min())\n",
    "    #print(y_data.tail(5))\n",
    "    #print(\"Target Variable Shape:\", y_data.shape[1])\n",
    "\n",
    "    # Drop target variable from features\n",
    "    x_data = x_data.drop(columns=[variable_y])\n",
    "    # Drop similar names\n",
    "    x_data = x_data.drop(columns=[col for col in x_data.columns if str(original_y) in col])\n",
    "    \n",
    "    print(\"Final Feature Engineered Data:\")\n",
    "    #print(x_scaled.index.min())\n",
    "    #print(y_data.index.min())\n",
    "\n",
    "    # Align indices of x_data and y_data\n",
    "    start_index = y_data.index[0]\n",
    "    end_index = y_data.index[-1]\n",
    "\n",
    "    x_data_filtered = x_data.loc[start_index:end_index]\n",
    "    \n",
    "    #print(\"Filtered Data Overview:\")\n",
    "    #print(x_data_filtered.index.min(), y_data.index.min())\n",
    "    #print(x_data_filtered.index.max(), y_data.index.max())\n",
    "    #print(\"Final Data Shapes:\", x_data_filtered.shape, y_data.shape)\n",
    "    # ========== SPLIT INTO TRAIN AND TEST ==========\n",
    "    #x_train, x_test, y_train, y_test = train_test_split(\n",
    "    #    x_data_filtered, y_data, test_size=test_size, random_state=random_state\n",
    "    #)\n",
    "\n",
    "    train_start=y_data.index[0]\n",
    "    train_end=y_data.index[-2]\n",
    "    test_start=y_data.index[-1]\n",
    "    test_end=y_data.index[-1]\n",
    "    print(f\"train_start{train_start}\")\n",
    "    print(f\"train_end{train_end}\")\n",
    "    print(f\"test_start{test_start}\")\n",
    "    print(f\"test_end{test_end}\")\n",
    "\n",
    "\n",
    "    x_train = x_data_filtered.loc[train_start:train_end]\n",
    "    x_test = x_data_filtered.loc[test_start:test_end]\n",
    "    y_train = y_data.loc[train_start:train_end]\n",
    "    y_test = y_data.loc[test_start:test_end]\n",
    "\n",
    "    #print(\"Training and Testing Data Split:\")\n",
    "    #print(\"Train:\", x_train.shape, y_train.shape)\n",
    "    #print(\"Test:\", x_test.shape, y_test.shape)\n",
    "    print(\"Complete\")\n",
    "    return {\n",
    "        \"x_train\": x_train,\n",
    "        \"x_test\": x_test,\n",
    "        \"y_train\": y_train,\n",
    "        \"y_test\": y_test,\n",
    "        \"y_data\": y_data\n",
    "    }\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "### 000 causality_analysis_function_step1\n",
    "\n",
    "def causality_analysis_function_step1(x_train, y_train, \n",
    "                                      rf_n_estimators=100, \n",
    "                                      xgb_n_estimators=100, \n",
    "                                      lgb_n_estimators=100,\n",
    "                                      random_state=99, \n",
    "                                      top_n=20, \n",
    "                                      size_width=10, \n",
    "                                      size_height=6,\n",
    "                                      vb_number=\"001\"):\n",
    "    feature_importance_results = {}\n",
    "\n",
    "    # ================== RANDOM FOREST ==================\n",
    "    #rfr rfr rfr\n",
    "    print(\"rfr\")\n",
    "    print(\"Training Random Forest Model...\")\n",
    "    rf_model = RandomForestRegressor(n_estimators=rf_n_estimators, random_state=random_state)\n",
    "    rf_model.fit(x_train, y_train)\n",
    "\n",
    "    rf_importance = pd.DataFrame({\n",
    "        'Feature': x_train.columns,\n",
    "        'Importance': rf_model.feature_importances_\n",
    "    }).sort_values(by='Importance', ascending=False)\n",
    "\n",
    "    rf_top_20 = rf_importance.head(top_n)\n",
    "    feature_importance_results['RandomForest'] = rf_top_20\n",
    "\n",
    "    plt.figure(figsize=(size_width, size_height))\n",
    "    plt.barh(rf_top_20['Feature'], rf_top_20['Importance'], color='skyblue')\n",
    "    plt.xlabel(f\"Importance \\n {variable_y}\",size=text_y)\n",
    "    plt.title(f\"Random Forest - Top 20 Features \\n {variable_y}\",size=text_title_s)\n",
    "    plt.gca().invert_yaxis()\n",
    "    # Reduce the size of y-axis tick labels\n",
    "    plt.yticks(fontsize=ticks_vertical)  # Adjust the fontsize as needed\n",
    "    plt.tight_layout()\n",
    "    #plt.savefig(f\"rf_top_20_features_{vb_number}.png\")\n",
    "    plt.savefig(f\"hv_tb_cuk_001_{vb_number}_m_g_001.png\")\n",
    "    #plt.show()\n",
    "    plt.close()\n",
    "    del rf_model\n",
    "    #del rf_importance\n",
    "    #del rf_top_20\n",
    "\n",
    "\n",
    "    \n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    print(\"--------------------------------------------------------------------------------------------------------------\")\n",
    "    # ================== XGBOOST ==================\n",
    "    print(\"Training XGBoost Model...\")\n",
    "    # Train XGBoost mode\n",
    "    xgb_model = XGBRegressor(n_estimators=xgb_n_estimators, random_state=random_state)\n",
    "    xgb_model.fit(x_train, y_train)\n",
    "\n",
    "    xgb_importance = pd.DataFrame({\n",
    "        'Feature': x_train.columns,\n",
    "        'Importance': xgb_model.feature_importances_\n",
    "    }).sort_values(by='Importance', ascending=False)\n",
    "\n",
    "    xgb_top_20 = xgb_importance.head(top_n)\n",
    "    feature_importance_results['XGBoost'] = xgb_top_20\n",
    "\n",
    "    plt.figure(figsize=(size_width, size_height))\n",
    "    plt.barh(xgb_top_20['Feature'], xgb_top_20['Importance'], color='orange')\n",
    "    plt.xlabel(f\"Importance \\n {variable_y}\",size=text_y)\n",
    "    plt.title(f\"XGBoost - Top 20 Features \\n {variable_y}\",size=text_title_s)\n",
    "    plt.gca().invert_yaxis()\n",
    "    plt.yticks(fontsize=ticks_vertical)  # Adjust the fontsize as needed\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(f\"xgb_top_20_features_{vb_number}.png\")\n",
    "    plt.savefig(f\"hv_tb_cuk_001_{vb_number}_m_g_003.png\")\n",
    "    #plt.show()\n",
    "    plt.close()\n",
    "    del xgb_model\n",
    "    #del xgb_importance\n",
    "    #del xgb_top_20\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    print(\"--------------------------------------------------------------------------------------------------------------\")\n",
    "    # ================== LIGHTGBM ==================\n",
    "    print(\"Training LightGBM Model...\")\n",
    "    lgb_model = lgb.LGBMRegressor(n_estimators=lgb_n_estimators, random_state=random_state)\n",
    "    lgb_model.fit(x_train, y_train)\n",
    "\n",
    "    lgb_importance = pd.DataFrame({\n",
    "        'Feature': x_train.columns,\n",
    "        'Importance': lgb_model.feature_importances_\n",
    "    }).sort_values(by='Importance', ascending=False)\n",
    "\n",
    "    lgb_top_20 = lgb_importance.head(top_n)\n",
    "    feature_importance_results['LightGBM'] = lgb_top_20\n",
    "\n",
    "    plt.figure(figsize=(size_width, size_height))\n",
    "    plt.barh(lgb_top_20['Feature'], lgb_top_20['Importance'], color='green')\n",
    "    plt.xlabel(f\"Importance \\n {variable_y}\",size=text_y)\n",
    "    plt.title(f\"LightGBM - Top 20 Features \\n {variable_y}\",size=text_title_s)\n",
    "    plt.gca().invert_yaxis()\n",
    "    plt.yticks(fontsize=ticks_vertical)  # Adjust the fontsize as needed\n",
    "    plt.tight_layout()\n",
    "    #plt.savefig(f\"lgb_top_20_features_{vb_number}.png\")\n",
    "    plt.savefig(f\"hv_tb_cuk_001_{vb_number}_m_g_004.png\")\n",
    "    #plt.show()\n",
    "    plt.close()\n",
    "    print(\"Complete\")\n",
    "    del lgb_model\n",
    "    #del lgb_importance\n",
    "    #del lgb_top_20\n",
    "\n",
    "\n",
    "   \n",
    "    print(\"Complete\")\n",
    "    return feature_importance_results\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "### 000 causality_analysis_function_step2\n",
    "\n",
    "\n",
    "\n",
    "def causality_analysis_function_step2(y_train, x_train, max_lag=10, top_n=15, size_width=10, size_height=6, vb_number=1):\n",
    " \n",
    "    # Function to perform Granger causality test for each variable\n",
    "    def granger_causality_for_variable(x_var, y, max_lag=10):\n",
    "        \"\"\"\n",
    "        Perform Granger causality test for a single x variable.\n",
    "        Returns the minimum p-value across lags and the optimal lag.\n",
    "        \"\"\"\n",
    "        data = pd.concat([y, x_var], axis=1).dropna()\n",
    "        results = grangercausalitytests(data, max_lag, verbose=False)\n",
    "        p_values = [results[lag][0]['ssr_ftest'][1] for lag in range(1, max_lag + 1)]\n",
    "        min_p_value = min(p_values)\n",
    "        optimal_lag = np.argmin(p_values) + 1  # Add 1 because lags are 1-indexed\n",
    "        return min_p_value, optimal_lag\n",
    "    \n",
    "    \n",
    "    y=y_train.copy()\n",
    "    x_variables=x_train.copy()\n",
    "    # Step 3: Apply the Granger causality test to each variable\n",
    "    results = []\n",
    "    max_lag = 10  # Maximum lag to test\n",
    "    for column in x_variables.columns:\n",
    "        #print(column)\n",
    "        x_var = x_variables[column]\n",
    "        p_value, optimal_lag = granger_causality_for_variable(x_var, y, max_lag)\n",
    "        results.append({'Variable': column,\n",
    "                        'P-Value': p_value, \n",
    "                        'Optimal Lag': optimal_lag})\n",
    "    \n",
    "    # Convert results to a DataFrame\n",
    "    results_df = pd.DataFrame(results)\n",
    "    # Apply negative logarithm transformation to p-values\n",
    "    results_df['-log(P-Value)'] = -np.log(results_df['P-Value'])\n",
    "    # Sort by transformed values\n",
    "    top_results = results_df.nlargest(15, '-log(P-Value)')\n",
    "    \n",
    "    \n",
    "    \n",
    "    # Plot the updated bar chart\n",
    "    plt.figure(figsize=(size_widht,size_height))\n",
    "    plt.barh(top_results['Variable'], top_results['-log(P-Value)'], color='green')\n",
    "    plt.title(f\"Top 15 Variables with Granger Causality to Y (-log(P-Value)) \\n {variable_y}\",size=text_title_s)\n",
    "    plt.xlabel(f\"-log(P-Value) (Higher is Better) \\n {variable_y}\",size=text_y)\n",
    "    plt.ylabel('Variable')\n",
    "    plt.gca().invert_yaxis()  # Ensuring the highest causality (highest -log(P-Value)) is at the top\n",
    "    plt.yticks(fontsize=ticks_vertical)  # Adjust the fontsize as needed\n",
    "    plt.tight_layout()\n",
    "    #plt.savefig('hv_tb_cuk_001_001_m_g_004.png')\n",
    "    #plt.savefig('hv_tb_cuk_001_001_m_g_006.png')\n",
    "    plt.savefig(f\"hv_tb_cuk_001_{vb_number}_m_g_005.png\")\n",
    "    # Display the plot\n",
    "    #plt.show()\n",
    "    plt.close()\n",
    "    print(\"Complete\")\n",
    "    return results_df\n",
    "\n",
    "### 000 causality_analysis_function_step3\n",
    "\n",
    "def causality_analysis_function_step3(y_train, \n",
    "                                      x_train, \n",
    "                                      max_lag=10, \n",
    "                                      top_n=15, \n",
    "                                      size_width=10, size_height=6, vb_number=1):\n",
    "#### Prompt \n",
    "#there is a dataframe A with a varaible y  (y_data). AND THERE is a dataframe B \n",
    "#WITH time series of difference economic varaibles x (x_data). i wanto to carry a granger causality \n",
    "#test to find the top 10 x variables that have the most causality to expalin (after finding the correct lag) \n",
    "#the variable Y. Then present the resutls in a vertical bar chart. Vertical bar chart. Try to do hyperparameter tuning or another adjustemnte\n",
    " \n",
    "    print(\"grannger statiosnaruty\")\n",
    "    # Step 1: Preprocess Data\n",
    "    y_train_gs=y_train.copy()\n",
    "    x_train_gs=x_train.copy()\n",
    "    data_gs = pd.concat([y_train_gs, x_train_gs], axis=1).dropna()\n",
    "    data_gs.replace([np.inf, -np.inf], 0, inplace=True)\n",
    "    data_gs.fillna(0, inplace=True)\n",
    "    \n",
    "    # Step 2: Function to test stationarity (ADF Test)\n",
    "    def test_stationarity(series):\n",
    "        \"\"\"Perform Augmented Dickey-Fuller (ADF) test and return the p-value.\"\"\"\n",
    "        result = adfuller(series)\n",
    "        return result[1]  # Extract the p-value of the test\n",
    "    \n",
    "    # Ensure stationarity for y_train\n",
    "    if test_stationarity(y_train_gs) > 0.05:\n",
    "        y_train_gs = y_train_gs.diff().dropna()\n",
    "    \n",
    "    # Step 3: Granger Causality Test with Optimal Lag Selection\n",
    "    def granger_test(y, x, max_lag=10):\n",
    "        \"\"\"Run Granger causality test for a given x variable on y and return best lag & p-value.\"\"\"\n",
    "        combined_data = pd.concat([y, x], axis=1).dropna()\n",
    "        test_result = grangercausalitytests(combined_data, max_lag, verbose=False)    \n",
    "        # Extract the best lag (based on p-value)\n",
    "        best_lag, best_pval = min(\n",
    "            [(lag, test[0]['ssr_ftest'][1]) for lag, test in test_result.items()],\n",
    "            key=lambda x: x[1]  # Sort by p-value (smallest = best)\n",
    "        )\n",
    "        return best_lag, best_pval\n",
    "    # Step 4: Loop through all x variables and perform Granger causality test\n",
    "    results = []\n",
    "    for col in x_train_gs.columns:\n",
    "        x_var = x_train_gs[col]   \n",
    "        # Ensure stationarity of x_var\n",
    "        if test_stationarity(x_var) > 0.05:\n",
    "            x_var = x_var.diff().dropna()\n",
    "        try:\n",
    "            lag, pval = granger_test(y_train_gs, x_var)\n",
    "            results.append({'Variable': col, 'P-Value': pval, 'Lag': lag})\n",
    "        except Exception as e:\n",
    "            print(f\"Error testing variable {col}: {e}\")\n",
    "    \n",
    "    # Step 5: Rank variables by -log(P-Value) (Causality Strength)\n",
    "    results_df = pd.DataFrame(results)\n",
    "    results_df['Causality Strength (-log(P-Value))'] = -np.log(results_df['P-Value'])\n",
    "    # **Sort variables by strongest causality (highest -log(P-Value))**\n",
    "    results_df = results_df.sort_values(by='Causality Strength (-log(P-Value))', ascending=False).head(20)\n",
    "    \n",
    "    # Step 6: Visualize Results with Sorted Bar Chart\n",
    "    plt.figure(figsize=(size_widht, size_height))\n",
    "    plt.barh(results_df['Variable'], results_df['Causality Strength (-log(P-Value))'], color='blue')\n",
    "    plt.title(f\"Top 15 Variables with Strongest Granger Causality \\n {variable_y}\",size=text_title_s)\n",
    "    plt.xlabel(f\"Causality Strength -log(P-Value) (Higher is Better) \\n {variable_y}\",size=text_y)\n",
    "    plt.ylabel('Variable')\n",
    "    plt.gca().invert_yaxis()  # Ensures strongest causality is at the **top**\n",
    "    plt.yticks(fontsize=ticks_vertical)  # Adjust the fontsize as needed\n",
    "    plt.xticks(rotation=45, ha='right')\n",
    "    plt.tight_layout()\n",
    "    #plt.savefig('hv_tb_cuk_001_001_m_g_007.png')\n",
    "    plt.savefig(f\"hv_tb_cuk_001_{vb_number}_m_g_006.png\")\n",
    "    # Display the plot\n",
    "    #plt.show()\n",
    "    plt.close()\n",
    "    print(\"Complete\")\n",
    "    #del results_df\n",
    "    return results_df\n",
    "\n",
    "#X-Axis (-log(P-Value))\n",
    "#Instead of plotting p-values directly, you are plotting -log(P-Value).\n",
    "#This transformation makes smaller p-values appear larger, which means:\n",
    "#A smaller p-value (strong causality) results in a taller bar.\n",
    "#A larger p-value (weaker causality) results in a shorter bar.\n",
    "\n",
    "### 000 causality_analysis_function_step4\n",
    "\n",
    "def causality_analysis_function_step4(y_train, x_train, max_iter=10, size_width=10, size_height=6, vb_number=1):\n",
    "    ### Stepwise regression \n",
    "    # Step 1: Preprocess the data\n",
    "    y_data_sr = y_train.copy()\n",
    "    x_data_sr = x_train.copy()\n",
    "    \n",
    "    # Step 2: Stepwise Regression with Forward Selection\n",
    "    def stepwise_regression(X, y, max_iter=10):\n",
    "        \"\"\"Perform forward selection stepwise regression.\"\"\"\n",
    "        selected_features = []\n",
    "        remaining_features = list(X.columns)\n",
    "        best_aic = np.inf\n",
    "    \n",
    "        while len(remaining_features) > 0 and max_iter > 0:\n",
    "            aic_values = []\n",
    "            \n",
    "            for feature in remaining_features:\n",
    "                current_features = selected_features + [feature]\n",
    "                model = sm.OLS(y, sm.add_constant(X[current_features])).fit()\n",
    "                aic_values.append((feature, model.aic))\n",
    "    \n",
    "            best_feature, best_model_aic = min(aic_values, key=lambda x: x[1])\n",
    "            \n",
    "            # Stop if no improvement in AIC\n",
    "            if best_model_aic < best_aic:\n",
    "                selected_features.append(best_feature)\n",
    "                remaining_features.remove(best_feature)\n",
    "                best_aic = best_model_aic\n",
    "                max_iter -= 1\n",
    "            else:\n",
    "                break  # No improvement in AIC\n",
    "    \n",
    "        # Return the selected features and the fitted model\n",
    "        final_model = sm.OLS(y, sm.add_constant(X[selected_features])).fit()\n",
    "        return selected_features, final_model\n",
    "    \n",
    "    # Step 3: Perform stepwise regression and get the final model\n",
    "    selected_vars, final_model = stepwise_regression(x_data_sr, y_data_sr)\n",
    "    # Step 4: Get the coefficients and p-values for the selected variables\n",
    "    coefficients = final_model.params[1:]  # Exclude constant\n",
    "    p_values = final_model.pvalues[1:]  # Exclude constant\n",
    "    # Step 5: Sort variables by p-values (best p-value on top)\n",
    "    sorted_indices = np.argsort(p_values)  # Sort in ascending order\n",
    "    sorted_vars = [selected_vars[i] for i in sorted_indices]  # Reorder variable names\n",
    "    sorted_p_values = [p_values[i] for i in sorted_indices]  # Reorder p-values\n",
    "    \n",
    "    \n",
    "    # Convert p-values to -log(p-value) for better visualization\n",
    "    neg_log_p_values = -np.log(sorted_p_values)\n",
    "    # Visualize the results with a properly sorted vertical bar plot\n",
    "    plt.figure(figsize=(size_widht, size_height))\n",
    "    plt.barh(sorted_vars, neg_log_p_values, color='skyblue', edgecolor='black')\n",
    "    plt.title(f\"Most Important Variables in Stepwise Regression (-log(p-value) Sorted) \\n {variable_y}\",size=text_title_s)\n",
    "    #plt.xlabel('-log(p-value)')\n",
    "    plt.xlabel(f\"Causality Strength -log(P-Value) (Higher is Better) \\n {variable_y}\",size=text_y)\n",
    "    plt.ylabel('Variable')\n",
    "    plt.gca().invert_yaxis()  # Ensure the best (smallest) p-value is on top\n",
    "    plt.yticks(fontsize=ticks_vertical)  # Adjust the fontsize as needed\n",
    "    plt.tight_layout()\n",
    "    # Save the image as PNG\n",
    "    #plt.savefig('hv_tb_cuk_001_001_m_g_008.png')\n",
    "    #plt.savefig('hv_tb_cuk_001_001_m_g_008a.png')\n",
    "    plt.savefig(f\"hv_tb_cuk_001_{vb_number}_m_g_007.png\")\n",
    "    # Show the plot\n",
    "    #plt.show()\n",
    "    plt.close()\n",
    "    print(\"Complete\")\n",
    "    \n",
    "    #del selected_vars\n",
    "    #del final_model\n",
    "    del coefficients\n",
    "    del p_values\n",
    "    del sorted_indices\n",
    "    del sorted_p_values\n",
    "    del sorted_vars\n",
    "    del neg_log_p_values\n",
    "    return selected_vars, final_model\n",
    "\n",
    "### 000 graph function \n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "\n",
    "def plot_time_series_with_correlation(\n",
    "    df, variable_y, select_x_variable, tail_size=60,title_r=None, y_label=\"Index %\", \n",
    "    apply_diff_y=True, apply_shift_y=True, apply_diff_x=True, apply_shift_x=True, rolling_window=12, \n",
    "    color_x=\"red\", color_y=\"black\",output_filename=\"plot.png\"\n",
    "):\n",
    "    \"\"\"\n",
    "    Plots a time series comparison of Y vs. X along with their 12-month rolling correlation.\n",
    "\n",
    "    Parameters:\n",
    "        df (pd.DataFrame): The input dataframe containing the variables.\n",
    "        variable_y (str): The dependent variable (Y) to be plotted on the left axis.\n",
    "        select_x_variable (str): The independent variable (X) to be plotted on the right axis.\n",
    "        title_r (str, optional): Custom title for the graph.\n",
    "        y_label (str, optional): Label for the Y-axis.\n",
    "        apply_diff (bool, optional): Apply first difference transformation.\n",
    "        apply_shift (bool, optional): Shift X-variable forward by one period.\n",
    "        rolling_window (int, optional): Window size for rolling correlation.\n",
    "        output_filename (str, optional): Name of the saved plot file.\n",
    "    \"\"\"\n",
    "\n",
    "    # Subset the relevant columns\n",
    "    df1 = df[[variable_y, select_x_variable]].copy()\n",
    "    df1 = df1.tail(tail_size)  # Keep last 60 observations\n",
    "\n",
    "    # Feature Engineering\n",
    "    if apply_diff_y:\n",
    "        df1[variable_y] = df1[variable_y].diff()\n",
    "\n",
    "    if apply_shift_y:\n",
    "        df1[select_x_variable] = df1[select_x_variable].shift(1)\n",
    "        \n",
    "    if apply_diff_x:    \n",
    "        df1[select_x_variable] = df1[select_x_variable].diff()\n",
    "\n",
    "    if apply_shift_x:\n",
    "        df1[select_x_variable] = df1[select_x_variable].shift(1)\n",
    "\n",
    "    \n",
    "\n",
    "    # Compute the rolling correlation\n",
    "    df_corr = df1[variable_y].rolling(window=rolling_window).corr(df1[select_x_variable])\n",
    "    df_corr = df_corr.dropna()\n",
    "\n",
    "    # Ensure index is in datetime format\n",
    "    df_corr.index = pd.to_datetime(df_corr.index)\n",
    "\n",
    "    # Default title if not provided\n",
    "    if title_r is None:\n",
    "        title_r = f\"Y: {variable_y} (black- left axis) vs X: {select_x_variable} (red - right axis)\"\n",
    "\n",
    "    # Create the figure with 2 rows, 1 column\n",
    "    fig, axs = plt.subplots(2, 1, figsize=(8, 8), sharex=True)\n",
    "\n",
    "    # Top plot: Time series comparison\n",
    "    axs[0].plot(df1.index, df1[variable_y], \n",
    "                color=color_y, linewidth=2, linestyle='-', marker='s', markersize=3, label=variable_y)\n",
    "    axs[0].set_ylabel(variable_y, color=color_y, fontsize=8)\n",
    "    axs[0].tick_params(axis='y', labelsize=6, colors=color_y)\n",
    "    axs[0].grid(axis='both', alpha=0.3)\n",
    "    \n",
    "    ax2 = axs[0].twinx()\n",
    "    ax2.plot(df1.index, df1[select_x_variable], \n",
    "             color=color_x, linewidth=2, linestyle='-', marker='>', markersize=3, label=select_x_variable)\n",
    "    ax2.set_ylabel(select_x_variable, color=color_x, fontsize=8)\n",
    "    ax2.tick_params(axis='y', labelsize=6, colors=color_x)\n",
    "    \n",
    "    axs[0].set_title(title_r, fontsize=10)\n",
    "\n",
    "    # Bottom plot: 12-month rolling correlation\n",
    "    axs[1].plot(df_corr.index, df_corr, color=\"blue\", linewidth=2, linestyle='-', marker='o', markersize=3)\n",
    "    axs[1].axhline(0, color='black', lw=1, linestyle='dashed', alpha=0.75)\n",
    "    axs[1].set_ylabel(\"12-Month Rolling Correlation\", color=\"blue\", fontsize=8)\n",
    "    axs[1].set_xlabel(\"Time\", fontsize=8)\n",
    "    axs[1].grid(axis='both', alpha=0.3)\n",
    "    axs[1].set_title(f\"12-Month Rolling Correlation: {variable_y} vs {select_x_variable}\", fontsize=10)\n",
    "\n",
    "    # Adjust layout and save the figure\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(output_filename, bbox_inches='tight')\n",
    "    plt.show()\n",
    "    plt.close(fig)\n",
    "    #del fig\n",
    "    #del ax\n",
    "    print(\"Complete\")\n",
    "    \n",
    "\n",
    "    print(f\"Plot saved as {output_filename}\")\n",
    "\n",
    "\n",
    "### --------------------------------------------------------------------------------------------------\n",
    "\n",
    "### 001 data loading api\n",
    "\n",
    "############# Monthly\n",
    "country_database = \"UK\"\n",
    "horizon_00=120\n",
    "uki_m_tickers1 = ['uksepep','ecy24', 's2ku4', \n",
    "                  #'ecy4', \n",
    "                  'j467', 'k222', \n",
    "                 # 'ugfccf',\n",
    "                  'ukseppfc', 'ukseppd',\n",
    "                 # 'kaj2', 'kaj5', \n",
    "                  'uknvai',\n",
    "                  'uhpaaps', 'ukshj','d7bt', \n",
    "                  'dkc6', 'd7f4', \n",
    "                  #'d7f5',\n",
    "                  'ukseppfc', 'ukseppfi', 'ukseppfo',\n",
    "                  'kac3', 'kai9',\n",
    "                 #'a3ww', 'a2fa',\n",
    "                  'chaw', 'chay', 'chax', 'chaz', \n",
    "                 'chmk', 'chby', 'chof', 'chba', \n",
    "                 'chbt', 'chol', 'chok', 'chog',\n",
    "                 'dobp', 'choh', 'choi', 'choj',\n",
    "                  'unol2t', 'unol1t', \n",
    "                 'unol3t', \n",
    "                  #'ugfci', \n",
    "                  #'ugfcfe', \n",
    "                 #'ugfcge', \n",
    "                  #'ugfcpe',\n",
    "                 #'ugfcm',\n",
    "                  #'ugfcs',\n",
    "           #       'uqvdse', 'uqvdse2m', \n",
    "           #      'uqvdseoo', 'uqvdseco', \n",
    "           #      'uqvdsewr', 'uqvdsetr',\n",
    "           #      'uqvdsefa', 'uqvdseij', \n",
    "           #      'uqvdsefi', 'uqvdserr',\n",
    "           #      'uqvdsebx', 'uqvdsebv', \n",
    "           #      'uqvdsehs', 'uqvdsezz',\n",
    "           #       'uqvdpe','uqvdpe2m',\n",
    "           #    'uqvdpeoo','uqvdpeco',\n",
    "           #    'uqvdpewr','uqvdpetr',\n",
    "           #    'uqvdpefa','uqvdpeij',\n",
    "           #    'uqvdpefi','uqvdperr',\n",
    "           #    'uqvdpebx','uqvdpebv',\n",
    "           #    'uqvdpehs','uqvdpezz',\n",
    "           #       'unvdrme', 'unvdrle', \n",
    "           #      'unvdran', 'unvdrlh',\n",
    "           #      'unvdrmh', 'unvdrn',\n",
    "                  's2ku4', 's2kv4', 's2kz4', \n",
    "                 's2l84', 's2lb4', 'ki7l4', \n",
    "                 's2lh4', 's2lm4', 's2lq4',\n",
    "                 's2lz4', 'ki7t4', 's2m84', \n",
    "                 's2ma4', 's2mc4', 's2mg4',\n",
    "                 's2ml4', 's2mp4',\n",
    "                  'uhpaaps','uhpaap','uhpaas','uhpaa','ukshrpi','ukshrap',\n",
    "              'uhpnbs','uhpnb','uhpnbp',\n",
    "              'unrmin','unrmap',\n",
    "              'uknprhm',\n",
    "             #'uknpipr','uknpiprm','uknpipra',\n",
    "              'ukspt','uksptn','uknpt','uknptn',\n",
    "              'urshp','urnhp','urnhpe','urnhe','urnhse','urnhx','urnhg','urnhq','urnhn',\n",
    "              'ukmvrdt','ukmvrli','ukmvrf3',\n",
    "             # 'ukncp','ukncpn','ukncpnh','ukncpni','ukncpnb','ukncpnp','ukncpnc','ukncpr','ukncprh','ukncprn'\n",
    "                 ]\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#x_data_input[\"ugfci\"]=x_data_input[\"ugfci\"]+ 100\n",
    "#x_data_input[\"ugfccf\"]=x_data_input[\"ugfccf\"]+ 100\n",
    "#x_data_input[\"ugfcfe\"]=x_data_input[\"ugfcfe\"]+ 100\n",
    "#x_data_input[\"ugfcge\"]=x_data_input[\"ugfcge\"]+ 100\n",
    "#x_data_input[\"ugfcpe\"]=x_data_input[\"ugfcpe\"]+ 100\n",
    "#x_data_input[\"ugfcm\"]=x_data_input[\"ugfcm\"]+ 100\n",
    "#x_data_input[\"ugfcs\"]=x_data_input[\"ugfcs\"]+ 100\n",
    "\n",
    "\n",
    "#### try to have the data in oringianl  index level form \n",
    "x_data_input = Haver.data(uki_m_tickers1, country_database, dates=True)\n",
    "x_data_input.rename(columns={\"uksepep\": \"paye_wholesale_sa\"}, inplace=True)\n",
    "x_data_input.rename(columns={\"ecy24\": \"real_monthly_gdp\"}, inplace=True)\n",
    "x_data_input.rename(columns={\"s2ku4\": \"real_service_industry\"}, inplace=True)\n",
    "x_data_input.rename(columns={\"ecy4\": \"real_production_industries\"}, inplace=True)\n",
    "x_data_input.rename(columns={\"j467\": \"real_retail_excl_fuel\"}, inplace=True)\n",
    "x_data_input.rename(columns={\"k222\": \"real_industrial_production\"}, inplace=True)\n",
    "x_data_input.rename(columns={\"ukseppfc\": \"employment_paye_payroll_change\"}, inplace=True)\n",
    "x_data_input.rename(columns={\"ukseppd\": \"employment_paye_median\"}, inplace=True)\n",
    "#x_data_input.rename(columns={\"kaj2\": \"employment_awe_private pay\"}, inplace=True)\n",
    "#x_data_input.rename(columns={\"kaj5\": \"employment_awe_regular pay\"}, inplace=True)\n",
    "x_data_input.rename(columns={\"uknvai\": \"employment_adzuna_job_advertisements\"}, inplace=True)\n",
    "x_data_input.rename(columns={\"d7bt\":\"price_cpi_headline\"}, inplace=True)\n",
    "x_data_input.rename(columns={\"dkc6\":\"price_cpi_core\"}, inplace=True)\n",
    "x_data_input.rename(columns={\"d7f4\":\"price_cpi_goods\"}, inplace=True)\n",
    "####>> prices cpi services cosntsnta\n",
    "#x_data_input.rename(columns={\"d7f5\":\"price_cpi_services\"}, inplace=True)\n",
    "x_data_input.rename(columns={\"uhpaaps\":\"price_halifax_house price\"}, inplace=True)\n",
    "x_data_input.rename(columns={\"ukshj\":\"price_hm_house_price\"}, inplace=True)\n",
    "\n",
    "x_data_input.rename(columns={\"m112vpmg\":\"cycle_pmi_manufacturing\"}, inplace=True)\n",
    "x_data_input.rename(columns={\"m112vpms\":\"cycle_pmi_services\"}, inplace=True)\n",
    "\n",
    "x_data_input.rename(columns={'ukseppfc': 'uk_hmrc_paye_payroll_change'}, inplace=True)\n",
    "x_data_input.rename(columns={'ukseppfi': 'hmrc_paye_payroll_inflows'}, inplace=True)\n",
    "x_data_input.rename(columns={'ukseppfo': 'hmrc_paye_payroll_outflows'}, inplace=True)#\n",
    "\n",
    "x_data_input.rename(columns={\"kac3\": \"awe_nominal_pay_sa_with_bonus\"}, inplace=True)\n",
    "x_data_input.rename(columns={\"kai9\": \"awe_nominal_pay_sa_no_bonus\"}, inplace=True)\n",
    "### cosntsnat \n",
    "#x_data_input.rename(columns={\"a3ww\": \"awe_real_pay_sa_with_bonus\"}, inplace=True)\n",
    "#x_data_input.rename(columns={\"a2fa\": \"awe_real_pay_sa_no_bonus\"}, inplace=True)\n",
    "# #'awe_real_pay_sa_no_bonus_diff1_fmt_mm_zscore24'\n",
    "# #'awe_real_pay_sa_no_bonus_diff1_fmt_yy_zscore24', \n",
    "\n",
    "x_data_input.rename(columns={\"kai7\": \"awe_whole_economy_regular_pay\"}, inplace=True)\n",
    "x_data_input.rename(columns={\"kaj2\": \"awe_private_sector_regular_pay\"}, inplace=True)\n",
    "x_data_input.rename(columns={\"kaj5\": \"awe_public_sector_regular_pay\"}, inplace=True)\n",
    "x_data_input.rename(columns={\"kak6\": \"awe_public_sector_excl_finance_regular_pay\"}, inplace=True)\n",
    "x_data_input.rename(columns={\"k5du\": \"awe_manufacturing_regular_pay\"}, inplace=True)\n",
    "x_data_input.rename(columns={\"k5dx\": \"awe_construction_regular_pay\"}, inplace=True)\n",
    "x_data_input.rename(columns={\"k5dl\": \"awe_services_regular_pay\"}, inplace=True)\n",
    "x_data_input.rename(columns={\"k5e2\": \"awe_wholesale_regular_pay\"}, inplace=True)\n",
    "x_data_input.rename(columns={\"k5do\": \"awe_finance_regular_pay\"}, inplace=True)\n",
    "\n",
    "\n",
    "\n",
    "x_data_input.rename(columns={\"ukseppm\":  \"uk_paye_mean_monthly_sa\"}, inplace=True)\n",
    "x_data_input.rename(columns={\"ukseppd\":  \"uk_paye_median monthly_sa\"}, inplace=True)\n",
    "x_data_input.rename(columns={\"ukseppp1\": \"uk_paye_10th_percentile_sa\"}, inplace=True)\n",
    "x_data_input.rename(columns={\"ukseppp2\": \"uk_paye_25th_percentile_sa\"}, inplace=True)\n",
    "x_data_input.rename(columns={\"ukseppp3\": \"uk_paye_50th_percentile_sa\"}, inplace=True)\n",
    "x_data_input.rename(columns={\"ukseppp4\": \"uk_paye_75th_percentile_sa\"}, inplace=True)\n",
    "x_data_input.rename(columns={\"ukseppp5\": \"uk_paye_90th_percentile_sa\"}, inplace=True)\n",
    "x_data_input.rename(columns={\"ukseppp6\": \"uk_paye_95th_percentile_sa\"}, inplace=True)\n",
    "x_data_input.rename(columns={\"ukseppp7\": \"uk_paye_99th_percentile_sa\"}, inplace=True)\n",
    "\n",
    "\n",
    "x_data_input.rename(columns={\"ukneppfi\": \"payroll_inflow_nsa\"}, inplace=True)\n",
    "x_data_input.rename(columns={\"ukneppfo\": \"payroll_outflow_nsa\"}, inplace=True)\n",
    "x_data_input.rename(columns={\"ukneppfc\": \"payroll_change_nsa\"}, inplace=True)\n",
    "x_data_input.rename(columns={\"uknepep\":  \"payroll_nsa\"}, inplace=True)\n",
    "x_data_input.rename(columns={\"uknepepa\": \"payroll_agriculture_nsa\"}, inplace=True)\n",
    "x_data_input.rename(columns={\"uknepepb\": \"payroll_mining_nsa\"}, inplace=True)\n",
    "x_data_input.rename(columns={\"uknepepc\": \"payroll_manufacturing_nsa\"}, inplace=True)\n",
    "x_data_input.rename(columns={\"uknepepd\": \"payroll_energy_nsa\"}, inplace=True)\n",
    "x_data_input.rename(columns={\"uknepepe\": \"payroll_water_nsa\"}, inplace=True)\n",
    "x_data_input.rename(columns={\"uknepepf\": \"payroll_construction_nsa\"}, inplace=True)\n",
    "x_data_input.rename(columns={\"uknepepg\": \"payroll_wholesale_nsa\"}, inplace=True)\n",
    "x_data_input.rename(columns={\"uknepeph\": \"payroll_transportation_nsa\"}, inplace=True)\n",
    "x_data_input.rename(columns={\"uknepepi\": \"payroll_accommodation_nsa\"}, inplace=True)\n",
    "x_data_input.rename(columns={\"uknepepj\": \"payroll_information_nsa\"}, inplace=True)\n",
    "x_data_input.rename(columns={\"uknepepk\": \"payroll_finance_nsa\"}, inplace=True)\n",
    "x_data_input.rename(columns={\"uknepepl\": \"payroll_real_estate_nsa\"}, inplace=True)\n",
    "x_data_input.rename(columns={\"uknepepm\": \"payroll_scientific_nsa\"}, inplace=True)\n",
    "x_data_input.rename(columns={\"uknepepn\": \"payroll_administrative_nsa\"}, inplace=True)\n",
    "x_data_input.rename(columns={\"uknepepo\": \"payroll_public_nsa\"}, inplace=True)\n",
    "x_data_input.rename(columns={\"uknepepp\": \"payroll_education_nsa\"}, inplace=True)\n",
    "x_data_input.rename(columns={\"uknepepq\": \"payroll_health_nsa\"}, inplace=True)\n",
    "x_data_input.rename(columns={\"uknepepr\": \"payroll_entertainment_nsa\"}, inplace=True)\n",
    "x_data_input.rename(columns={\"uknepeps\": \"payroll_other_nsa\"}, inplace=True)\n",
    "x_data_input.rename(columns={\"uknepept\": \"payroll_household_nsa\"}, inplace=True)\n",
    "\n",
    "\n",
    "x_data_input.rename(columns={\"ukseppfi\": \"payroll_inflow_sa\"}, inplace=True)\n",
    "x_data_input.rename(columns={\"ukseppfo\": \"payroll_outflow_sa\"}, inplace=True)\n",
    "x_data_input.rename(columns={\"ukseppfc\": \"payroll_change_sa\"}, inplace=True)\n",
    "x_data_input.rename(columns={\"uksepep\":  \"payroll_sa\"}, inplace=True)\n",
    "x_data_input.rename(columns={\"uksepepa\": \"payroll_agriculture_sa\"}, inplace=True)\n",
    "x_data_input.rename(columns={\"uksepepb\": \"payroll_mining_sa\"}, inplace=True)\n",
    "x_data_input.rename(columns={\"uksepepc\": \"payroll_manufacturing_sa\"}, inplace=True)\n",
    "x_data_input.rename(columns={\"uksepepd\": \"payroll_energy_sa\"}, inplace=True)\n",
    "x_data_input.rename(columns={\"uksepepe\": \"payroll_water_sa\"}, inplace=True)\n",
    "x_data_input.rename(columns={\"uksepepf\": \"payroll_construction_sa\"}, inplace=True)\n",
    "x_data_input.rename(columns={\"uksepepg\": \"payroll_wholesale_sa\"}, inplace=True)\n",
    "x_data_input.rename(columns={\"uksepeph\": \"payroll_transportation_sa\"}, inplace=True)\n",
    "x_data_input.rename(columns={\"uksepepi\": \"payroll_accommodation_sa\"}, inplace=True)\n",
    "x_data_input.rename(columns={\"uksepepj\": \"payroll_information_sa\"},inplace=True)\n",
    "x_data_input.rename(columns={\"uksepepk\": \"payroll_finance_sa\"}, inplace=True)\n",
    "x_data_input.rename(columns={\"uksepepl\": \"payroll_real_estate_sa\"}, inplace=True)\n",
    "x_data_input.rename(columns={\"uksepepm\": \"payroll_scientific_sa\"},inplace=True)\n",
    "x_data_input.rename(columns={\"uksepepn\": \"payroll_administrative_sa\"}, inplace=True)\n",
    "x_data_input.rename(columns={\"uksepepo\": \"payroll_public_sa\"},inplace=True)\n",
    "x_data_input.rename(columns={\"uksepepp\": \"payroll_education_sa\"},inplace=True)\n",
    "x_data_input.rename(columns={\"uksepepq\": \"payroll_health_sa\"},inplace=True)\n",
    "x_data_input.rename(columns={\"uksepepr\": \"payroll_entertainment_sa\"}, inplace=True)\n",
    "x_data_input.rename(columns={\"uksepeps\": \"payroll_other_sa\"}, inplace=True)\n",
    "x_data_input.rename(columns={\"uksepept\": \"payroll_household_sa\"}, inplace=True)\n",
    "\n",
    "x_data_input.rename(columns={'kai9': 'uk_awe_regular_nominal_yy_nsa_monthly'}, inplace=True)\n",
    "x_data_input.rename(columns={'ukseurt': 'uk_lfs_unemployment_rate_3m_sa_monthly'}, inplace=True)\n",
    "\n",
    "x_data_input.rename(columns={\"d7bt\": \"cpi_all_items_yy_nsa_2015_100\"}, inplace=True)\n",
    "x_data_input.rename(columns={\"dkc6\": \"cpi_core_nsa_yy\"}, inplace=True)\n",
    "x_data_input.rename(columns={\"d7f4\": \"cpi_goods_nsa_yy\"}, inplace=True)\n",
    "####>>> constant\n",
    "#x_data_input.rename(columns={\"d7f5\": \"cpi_services nsa yy\"}, inplace=True)\n",
    "x_data_input.rename(columns={\"d7bu\": \"cpi_food_&_nonalcoholic_beverages_nsa_yy\"}, inplace=True)\n",
    "x_data_input.rename(columns={\"d7bv\": \"cpi_alcoholic_beverages_&_tobacco_nsa_yy\"}, inplace=True)\n",
    "x_data_input.rename(columns={\"d7bw\": \"cpi_clothing_&_footwear_nsa_yy\"}, inplace=True)\n",
    "x_data_input.rename(columns={\"d7bx\": \"cpi_housing_water_electricity_gas_nsa_yy\"}, inplace=True)\n",
    "x_data_input.rename(columns={\"d7by\": \"cpi_furniture_household_equipment_uk_nsa_yy\"}, inplace=True)\n",
    "x_data_input.rename(columns={\"d7bz\": \"cpi_healthuk_nsa_yy\"}, inplace=True)\n",
    "x_data_input.rename(columns={\"d7c2\": \"cpi_transport_nsa_yy\"}, inplace=True)\n",
    "x_data_input.rename(columns={\"d7c3\": \"cpi_communication_nsa_yy\"}, inplace=True)\n",
    "x_data_input.rename(columns={\"d7c4\": \"cpi_recreation_&_culture_nsa_yy\"}, inplace=True)\n",
    "x_data_input.rename(columns={\"d7c5\": \"cpi_education_nsa_yy\"}, inplace=True)\n",
    "x_data_input.rename(columns={\"d7c6\": \"cpi_hotels_cafes_& restaurants_nsa_yy\"}, inplace=True)\n",
    "x_data_input.rename(columns={\"d7c7\": \"cpi_miscellaneous_goods_&_services_nsa_yy\"}, inplace=True)\n",
    "\n",
    "x_data_input.rename(columns={\"chaw\": \"rpi_all_items\"}, inplace=True)\n",
    "x_data_input.rename(columns={\"chay\": \"rpi_excluding_food\"}, inplace=True)\n",
    "x_data_input.rename(columns={\"chax\": \"rpi_excluding_seasonal_food\"}, inplace=True)\n",
    "x_data_input.rename(columns={\"chaz\": \"rpi_excluding_housing\"}, inplace=True)\n",
    "x_data_input.rename(columns={\"chmk\": \"rpi_excluding_mortgage_interest_payments_rpix\"}, inplace=True)\n",
    "x_data_input.rename(columns={\"chby\": \"rpi_consumer_durable\"}, inplace=True)\n",
    "x_data_input.rename(columns={\"chof\": \"rpi_all_goods\"}, inplace=True)\n",
    "x_data_input.rename(columns={\"chba\": \"rpi_food\"}, inplace=True)\n",
    "x_data_input.rename(columns={\"chbt\": \"rpi_alcohol_&_tobacco\"}, inplace=True)\n",
    "x_data_input.rename(columns={\"chol\": \"rpi_petrol_and_oil\"}, inplace=True)\n",
    "x_data_input.rename(columns={\"chok\": \"rpi_other_goods\"}, inplace=True)\n",
    "x_data_input.rename(columns={\"chog\": \"rpi_all_services\"}, inplace=True)\n",
    "x_data_input.rename(columns={\"dobp\": \"rpi_rent\"}, inplace=True)\n",
    "x_data_input.rename(columns={\"choh\": \"rpi_utilities\"}, inplace=True)\n",
    "x_data_input.rename(columns={\"choi\": \"rpi_shop_services\"}, inplace=True)\n",
    "x_data_input.rename(columns={\"choj\": \"rpi_nonshop_services\"}, inplace=True)\n",
    "\n",
    "\n",
    "#x_data_input[\"unol2t\"]=x_data_input[\"unol2t\"]+ 100\n",
    "#x_data_input[\"unol1t\"]=x_data_input[\"unol1t\"]+ 100\n",
    "#x_data_input[\"unol3t\"]=x_data_input[\"unol3t\"]+ 100\n",
    "#x_data_input[\"ugfci\"]=x_data_input[\"ugfci\"]+ 100\n",
    "#x_data_input[\"ugfccf\"]=x_data_input[\"ugfccf\"]+ 100\n",
    "#x_data_input[\"ugfcfe\"]=x_data_input[\"ugfcfe\"]+ 100\n",
    "#x_data_input[\"ugfcge\"]=x_data_input[\"ugfcge\"]+ 100\n",
    "#x_data_input[\"ugfcpe\"]=x_data_input[\"ugfcpe\"]+ 100\n",
    "#x_data_input[\"ugfcm\"]=x_data_input[\"ugfcm\"]+ 100\n",
    "#x_data_input[\"ugfcs\"]=x_data_input[\"ugfcs\"]+ 100\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "x_data_input.rename(columns={\"unol2t\": \"cycle_lloyds_business_barometer_vs_3m_nsa\"}, inplace=True)\n",
    "x_data_input.rename(columns={\"unol1t\": \"cycle_lloyds_activity_next_12m_nsa\"}, inplace=True)\n",
    "x_data_input.rename(columns={\"unol3t\": \"cycle_lloyds_overall_bs_confidence_nsa\"}, inplace=True)\n",
    "x_data_input.rename(columns={\"ugfci\": \"cycle_gfk_consumer_confidence_barometer\"}, inplace=True)\n",
    "x_data_input.rename(columns={\"ugfccf\": \"cycle_gfk_current_financial_situation_hh_nsa\"}, inplace=True)\n",
    "x_data_input.rename(columns={\"ugfcfe\": \"cycle_gfk_hh_finance_nxt_12m_nsa\"}, inplace=True)\n",
    "x_data_input.rename(columns={\"ugfcge\": \"cycle_gfk_economic_situation_nxt_12m_nsa\"}, inplace=True)\n",
    "x_data_input.rename(columns={\"ugfcpe\": \"cycle_gfk_consumer_prices_nxt_12m_nsa\"}, inplace=True)\n",
    "x_data_input.rename(columns={\"ugfcm\": \"cycle_gfk_major_purchases_nsa\"}, inplace=True)\n",
    "x_data_input.rename(columns={\"ugfcs\": \"cycle_gfk_savings_nsa\"}, inplace=True)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "x_data_input.rename(columns={\"uqvdse\": \"boe_dmp_growth_sales_revenue_1y_nsa\"}, inplace=True)\n",
    "x_data_input.rename(columns={\"uqvdse2m\": \"boe_dmp_exp_growth_sales_manufacturing_1y_nsa\"}, inplace=True)\n",
    "x_data_input.rename(columns={\"uqvdseoo\": \"boe_dmp_exp_growth_sales_other_production_1y_nsa\"}, inplace=True)\n",
    "x_data_input.rename(columns={\"uqvdseco\": \"boe_dmp_exp_growth_sales_construction_1y_nsa\"}, inplace=True)\n",
    "x_data_input.rename(columns={\"uqvdsewr\": \"boe_dmp_exp_growth_sales_retail_1y_nsa\"}, inplace=True)\n",
    "x_data_input.rename(columns={\"uqvdsetr\": \"boe_dmp_exp_growth_sales_transport_1y_nsa\"}, inplace=True)\n",
    "x_data_input.rename(columns={\"uqvdsefa\": \"boe_dmp_exp_growth_sales_accommodation_1y_nsa\"}, inplace=True)\n",
    "x_data_input.rename(columns={\"uqvdseij\": \"boe_dmp_exp_growth_sales_communication_1y_nsa\"}, inplace=True)\n",
    "x_data_input.rename(columns={\"uqvdsefi\": \"boe_dmp_exp_growth_sales_finance_1y_nsa\"}, inplace=True)\n",
    "x_data_input.rename(columns={\"uqvdserr\": \"boe_dmp_exp_growth_sales_real_estate_1y_nsa\"}, inplace=True)\n",
    "x_data_input.rename(columns={\"uqvdsebx\": \"boe_dmp_exp_growth_sales_scientific_1y_nsa\"}, inplace=True)\n",
    "x_data_input.rename(columns={\"uqvdsebv\": \"boe_dmp_exp_growth_sales_admin_1y_nsa\"}, inplace=True)\n",
    "x_data_input.rename(columns={\"uqvdsehs\": \"boe_dmp_exp_growth_sales_health_1y_nsa\"}, inplace=True)\n",
    "x_data_input.rename(columns={\"uqvdsezz\": \"boe_dmp_exp_growth_sales_other_services_1y_nsa\"}, inplace=True)\n",
    "\n",
    "x_data_input.rename(columns={'uqvdpe':'boe_dmp_growth_prices_1y_nsa'},inplace=True)\n",
    "x_data_input.rename(columns={'uqvdpe2m':'boe_dmp_exp_growth_prices_manufacturing_1y_nsa'},inplace=True)\n",
    "x_data_input.rename(columns={'uqvdpeoo':'boe_dmp_exp_growth_prices_other production_1y_nsa'},inplace=True)\n",
    "x_data_input.rename(columns={'uqvdpeco':'boe_dmp_exp_growth_prices_construction_1y_nsa'},inplace=True)\n",
    "x_data_input.rename(columns={'uqvdpewr':'boe_dmp_exp_growth_prices_retail_1y_nsa'},inplace=True)\n",
    "x_data_input.rename(columns={'uqvdpetr':'boe_dmp_exp_growth_prices_transport_1y_nsa'},inplace=True)\n",
    "x_data_input.rename(columns={'uqvdpefa':'boe_dmp_exp_growth_prices_accommodation_1y_nsa'},inplace=True)\n",
    "x_data_input.rename(columns={'uqvdpeij':'boe_dmp_exp_growth_prices_communication_1y_nsa'},inplace=True)\n",
    "x_data_input.rename(columns={'uqvdpefi':'boe_dmp_exp_growth_prices_finance_1y_nsa'},inplace=True)\n",
    "x_data_input.rename(columns={'uqvdperr':'boe_dmp_exp_growth_prices_real estate_1y_nsa'},inplace=True)\n",
    "x_data_input.rename(columns={'uqvdpebx':'boe_dmp_exp_growth_prices_scientific_1y_nsa'},inplace=True)\n",
    "x_data_input.rename(columns={'uqvdpebv':'boe_dmp_exp_growth_prices_admin_ly_nsa'},inplace=True)\n",
    "x_data_input.rename(columns={'uqvdpehs':'boe_dmp_exp_growth_prices_health_ly_nsa'},inplace=True)\n",
    "x_data_input.rename(columns={'uqvdpezz':'boe_dmp_exp_growth_prices_other_services_1y_nsa'},inplace=True)\n",
    "\n",
    "\n",
    "x_data_input.rename(columns={\"unvdrme\": \"boe_dmp_hiring_difficulty_much_easier\"}, inplace=True)\n",
    "x_data_input.rename(columns={\"unvdrle\": \"boe_dmp_hiring_difficulty_a_little_easier\"}, inplace=True)\n",
    "x_data_input.rename(columns={\"unvdran\": \"boe_dmp_hiring_difficulty_about_normal\"}, inplace=True)\n",
    "x_data_input.rename(columns={\"unvdrlh\": \"boe_dmp_hiring_difficulty_a_little_harder\"}, inplace=True)\n",
    "x_data_input.rename(columns={\"unvdrmh\": \"boe_dmp_hiring_difficulty_much_harder\"}, inplace=True)\n",
    "x_data_input.rename(columns={\"unvdrn\": \"boe_dmp_hiring_difficulty_not_recruiting\"}, inplace=True)\n",
    "\n",
    "x_data_input.rename(columns={\"s2ku4\": \"uk_service_industry_sa\"}, inplace=True)\n",
    "x_data_input.rename(columns={\"s2kv4\": \"uk_wholesale_retail_sa\"}, inplace=True)\n",
    "x_data_input.rename(columns={\"s2kz4\": \"uk_transportation_sa\"}, inplace=True)\n",
    "x_data_input.rename(columns={\"s2l84\": \"uk_accommodation_food_service_sa\"}, inplace=True)\n",
    "x_data_input.rename(columns={\"s2lb4\": \"uk_information_communication_sa\"}, inplace=True)\n",
    "x_data_input.rename(columns={\"ki7l4\": \"uk_business_services_finance_sa\"}, inplace=True)\n",
    "x_data_input.rename(columns={\"s2lh4\": \"uk_finance_insurance_sa\"}, inplace=True)\n",
    "x_data_input.rename(columns={\"s2lm4\": \"uk_real_estate_sa\"}, inplace=True)\n",
    "x_data_input.rename(columns={\"s2lq4\": \"uk_scientific_sa\"}, inplace=True)\n",
    "x_data_input.rename(columns={\"s2lz4\": \"uk_admin_serv_sa\"}, inplace=True)\n",
    "x_data_input.rename(columns={\"ki7t4\": \"uk_government_other_sa\"}, inplace=True)\n",
    "x_data_input.rename(columns={\"s2m84\": \"uk_public_administration_sa\"}, inplace=True)\n",
    "x_data_input.rename(columns={\"s2ma4\": \"uk_education_sa\"}, inplace=True)\n",
    "x_data_input.rename(columns={\"s2mc4\": \"uk_human_health_sa\"}, inplace=True)\n",
    "x_data_input.rename(columns={\"s2mg4\": \"uk_recreation_sa\"}, inplace=True)\n",
    "x_data_input.rename(columns={\"s2ml4\": \"uk_other_service_sa\"}, inplace=True)\n",
    "x_data_input.rename(columns={\"s2mp4\": \"uk_domestic_personnel_sa\"}, inplace=True)\n",
    "\n",
    "\n",
    "x_data_input.rename(columns={'j5ek': 'retail_sales_incl_fuel_mm_sa'}, inplace=True)\n",
    "x_data_input.rename(columns={'j467': 'retail_sales_excl_fuel_mm_sa'}, inplace=True)\n",
    "x_data_input.rename(columns={'eapt': 'retail_food_stores_mm_sa'}, inplace=True)\n",
    "x_data_input.rename(columns={'eapv': 'retail_non_food_stores_mm_sa'}, inplace=True)\n",
    "x_data_input.rename(columns={'eapu': 'retail_non_specialized_stores_mm_sa'}, inplace=True)\n",
    "x_data_input.rename(columns={'eapx': 'retail_textile_clothing_mm_sa'}, inplace=True)\n",
    "x_data_input.rename(columns={'eapy': 'retail_household_goods_mm_sa'}, inplace=True)\n",
    "x_data_input.rename(columns={'eapw': 'retail_other_nonfood_stores_mm_sa'}, inplace=True)\n",
    "x_data_input.rename(columns={'j5dz': 'retail_nonstore_retailing_mm_sa'}, inplace=True)\n",
    "x_data_input.rename(columns={'jo5a': 'retail_automotive_fuel_mm_sa'}, inplace=True)\n",
    "\n",
    "x_data_input.rename(columns={\"ecy24\": \"uk_monthly_gdp_sa_yy\"}, inplace=True)\n",
    "x_data_input.rename(columns={\"ecy34\": \"uk_monthly_gdp_agriculture_fishing_sa_yy\"}, inplace=True)\n",
    "#iemployment_awe_private_paynplace=True)\n",
    "x_data_input.rename(columns={\"ecy44\": \"uk_monthly_gdp_production_sa_yy\"}, inplace=True)\n",
    "x_data_input.rename(columns={\"ecy94\": \"uk_monthly_gdp_construction_sa_yy\"}, inplace=True)\n",
    "x_data_input.rename(columns={\"ecyc4\": \"uk_monthly_gdp_services_sa_yy\"}, inplace=True)\n",
    "\n",
    "\n",
    "x_data_input.rename(columns={\"k222\": \"uk_ip_sa_yy\"}, inplace=True)\n",
    "#hv_tb_uk_011_001_m.rename(columns={\"k240\":\"uk ip-intermediate goods sa y/y\"}, inplace=True)\n",
    "x_data_input.rename(columns={\"k245\": \"uk_ip_capital_goods_sa_yy\"}, inplace=True)\n",
    "x_data_input.rename(columns={\"k24q\": \"uk_ip_durable_goods_sa_yy\"}, inplace=True)\n",
    "x_data_input.rename(columns={\"k24r\": \"uk_ip_nondurable_goods_sa_yy\"}, inplace=True)\n",
    "x_data_input.rename(columns={\"k24t\": \"uk_ip_energy_sa_yy\"}, inplace=True)\n",
    "\n",
    "x_data_input.rename(columns={\"s2ku4\": \"uk_service_industry_sa\"}, inplace=True)\n",
    "x_data_input.rename(columns={\"s2kv4\": \"wholesale_retail_sa\"}, inplace=True)\n",
    "x_data_input.rename(columns={\"s2kz4\": \"transportation_sa\"}, inplace=True)\n",
    "x_data_input.rename(columns={\"s2l84\": \"accommodation_food_service_sa\"}, inplace=True)\n",
    "x_data_input.rename(columns={\"s2lb4\": \"information_communication_sa\"}, inplace=True)\n",
    "x_data_input.rename(columns={\"ki7l4\": \"business_services_finance_sa\"}, inplace=True)\n",
    "x_data_input.rename(columns={\"s2lh4\": \"finance_insurance_sa\"}, inplace=True)\n",
    "x_data_input.rename(columns={\"s2lm4\": \"real_estate_sa\"}, inplace=True)\n",
    "x_data_input.rename(columns={\"s2lq4\": \"scientific_sa\"}, inplace=True)\n",
    "x_data_input.rename(columns={\"s2lz4\": \"admin_serv_sa\"}, inplace=True)\n",
    "x_data_input.rename(columns={\"ki7t4\": \"government_other_sa\"}, inplace=True)\n",
    "x_data_input.rename(columns={\"s2m84\": \"public_administration_sa\"}, inplace=True)\n",
    "x_data_input.rename(columns={\"s2ma4\": \"education_sa\"}, inplace=True)\n",
    "x_data_input.rename(columns={\"s2mc4\": \"human_health_sa\"}, inplace=True)\n",
    "x_data_input.rename(columns={\"s2mg4\": \"recreation_sa\"}, inplace=True)\n",
    "x_data_input.rename(columns={\"s2m14\": \"other_service_sa\"}, inplace=True)\n",
    "x_data_input.rename(columns={\"s2mp4\":\"domestic_personnel_sa\"},inplace=True)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "x_data_input.rename(columns={'uhpaaps':'housing_halifax_standard_avg_price_sa'}, inplace=True)\n",
    "x_data_input.rename(columns={'uhpaap':'housing_halifax_standard_avg_price_nsa'}, inplace=True)\n",
    "x_data_input.rename(columns={'uhpaas':'housing_halifax_index_price_sa'}, inplace=True)\n",
    "x_data_input.rename(columns={'uhpaa':'housing_halifax_index_price_nsa'}, inplace=True)\n",
    "x_data_input.rename(columns={'ukshrpi':'housing_halifax_rpi_sa'}, inplace=True)\n",
    "x_data_input.rename(columns={'ukshrap':'housing_halifax_real_avg_price_sa'}, inplace=True)\n",
    "x_data_input.rename(columns={'uhpnbs':'housing_nationwide_hpi_sa'}, inplace=True)\n",
    "x_data_input.rename(columns={'uhpnb':'housing_nationwide_hpi_nsa'}, inplace=True)\n",
    "x_data_input.rename(columns={'uhpnbp':'housing_nationwide_avg_houseprice_nsa'}, inplace=True)\n",
    "x_data_input.rename(columns={'unrmin':'housing_rightmove_hpi_nsa'}, inplace=True)\n",
    "x_data_input.rename(columns={'unrmap':'housing_rightmove_avg_asking_price_nsa'}, inplace=True)\n",
    "x_data_input.rename(columns={'uknprhm':'housing_mix_adg_avg_house_rpinsa'}, inplace=True)\n",
    "#x_data_input.rename(columns={'uknpipr':'housing_pipr_private_lv_nsa'}, inplace=True)\n",
    "#x_data_input.rename(columns={'uknpiprm':'housing_pipr_private_mm_nsa'}, inplace=True)\n",
    "#x_data_input.rename(columns={'uknpipra':'housing_housing_pipr_private_yy_nsa'}, inplace=True)\n",
    "x_data_input.rename(columns={'ukspt':'housing_res_property_transactions_sa'}, inplace=True)\n",
    "x_data_input.rename(columns={'uksptn':'housing_nonres_property_transactions_sa'}, inplace=True)\n",
    "x_data_input.rename(columns={'uknpt':'housing_res_property_transactions_nsa'}, inplace=True)\n",
    "x_data_input.rename(columns={'uknptn':'housing_nonres_property_transactions_nsa'}, inplace=True)\n",
    "x_data_input.rename(columns={'urshp':'housing_change_price_l3m_sa'}, inplace=True)\n",
    "x_data_input.rename(columns={'urnhp':'housing_change_price_l3m_nsa'}, inplace=True)\n",
    "x_data_input.rename(columns={'urnhpe':'housing_change_price_exp_n3m_nsa'}, inplace=True)\n",
    "x_data_input.rename(columns={'urnhe':'housing_change_price_exp_n12m_nsa'}, inplace=True)\n",
    "x_data_input.rename(columns={'urnhse':'housing_change_sales_exp_n3m_nsa'}, inplace=True)\n",
    "x_data_input.rename(columns={'urnhx':'housing_change_sales_exp_n12m_nsa'}, inplace=True)\n",
    "x_data_input.rename(columns={'urnhg':'housing_change_new_agreed_sales_nsa'}, inplace=True)\n",
    "x_data_input.rename(columns={'urnhq':'housing_change_new_buyer_nsa'}, inplace=True)\n",
    "x_data_input.rename(columns={'urnhn':'housing_change_new_instructions_nsa'}, inplace=True)\n",
    "x_data_input.rename(columns={'ukmvrdt':'housing_rics_res_tenant_demand_nsa'}, inplace=True)\n",
    "x_data_input.rename(columns={'ukmvrli':'housing_rics_res_tenant_landlord_instructions_nsa'}, inplace=True)\n",
    "x_data_input.rename(columns={'ukmvrf3':'housing_rics_res_rent_exp_nsa'}, inplace=True)\n",
    "#x_data_input.rename(columns={'ukncp':'housing_construction_output_allwork_nsa'}, inplace=True)\n",
    "#x_data_input.rename(columns={'ukncpn':'housing_construction_output_newwork_nsa'}, inplace=True)\n",
    "#x_data_input.rename(columns={'ukncpnh':'housing_construction_output_housing_nsa'}, inplace=True)\n",
    "#x_data_input.rename(columns={'ukncpni':'housing_construction_output_infrastructure_nsa'}, inplace=True)\n",
    "#x_data_input.rename(columns={'ukncpnb':'housing_construction_output_public_nsa'}, inplace=True)\n",
    "#x_data_input.rename(columns={'ukncpnp':'housing_construction_output_privatei_nsa'}, inplace=True)\n",
    "#x_data_input.rename(columns={'ukncpnc':'housing_construction_output_privatec_nsa'}, inplace=True)\n",
    "#x_data_input.rename(columns={'ukncpr':'housing_construction_output_repair_nsa'}, inplace=True)\n",
    "#x_data_input.rename(columns={'ukncprh':'housing_construction_output_repairh_nsa'}, inplace=True)\n",
    "#x_data_input.rename(columns={'ukncprn':'housing_construction_output_repairnh_nsa'}, inplace=True)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# #'price_cpi_services_diff1_fmt_mm_zscore24'\n",
    "# #'awe_real_pay_sa_no_bonus_diff1_fmt_mm_zscore24'\n",
    "# #'awe_real_pay_sa_no_bonus_diff1_fmt_yy_zscore24', \n",
    "#['employment_awe_private_pay_diff1_fmt_yy_mavg24'\n",
    "# #'employment_awe_regular_pay_diff1_fmt_yy_zs\n",
    " #['employment_awe_private_pay_diff1_fmt_yy_mavg24'\n",
    "# #'employment_awe_regular_pay_diff1_fmt_yy_zs\n",
    "\n",
    "#housing_pipr_private_mm_\n",
    "#ycle_gfk_current_financial_situation\n",
    "#housing_construction_output_repairh_nsa_rolling_avg3_diff1_fmt_qq_std_dev24_lag1\n",
    "\n",
    "\n",
    "x_data_input=x_data_input[x_data_input.index>=\"2010-01-01\"]\n",
    "\n",
    "print(x_data_input.index.min())\n",
    "print(x_data_input.index.max())\n",
    "\n",
    "print(x_data_input.shape[0])\n",
    "print(x_data_input.shape[1])\n",
    "print(x_data_input.columns.tolist())\n",
    "\n",
    "##### DOuble check data is in monthyl basisi  \n",
    "print(x_data_input.tail(12))\n",
    "\n",
    "print(x_data_input.columns.tolist())\n",
    "#a =x_data_input[[\"cycle_lloyds overall bs confidence nsa\"]]\n",
    "\n",
    "#Stop\n",
    "\n",
    "### 001 processing of feature engineering  \n",
    "\n",
    "processed_data = feature_engineering(x_data_input,   lags=[1, 2,])\n",
    "x_input_data_proc = processed_data[\"x_featured_data\"]\n",
    "x_input_data_scaled = processed_data[\"x_scaled_data\"]\n",
    "del x_data_input\n",
    "# 5/5 [01:49<00:00, 21.81s/it]\n",
    "\n",
    "print(x_input_data_proc.index.min())\n",
    "print(x_input_data_proc.index.max())\n",
    "print(x_input_data_proc.shape[0])\n",
    "print(x_input_data_proc.shape[1])\n",
    "\n",
    "constant_columns = [col for col in x_input_data_proc.columns if x_input_data_proc[col].nunique() == 1]\n",
    "print(constant_columns)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#save parquet \n",
    "x_input_data_proc.to_parquet(\"x_input_data_proc.parquet\", engine=\"pyarrow\", index=False)\n",
    "print(\"Complete\")\n",
    "x_input_data_scaled.to_parquet(\"x_input_data_scaled.parquet\", engine=\"pyarrow\", index=False)\n",
    "print(\"Complete\")\n",
    "\n",
    "print(x_input_data_proc.index.min())\n",
    "print(x_input_data_proc.index.max())\n",
    "print(x_input_data_proc.shape[0])\n",
    "print(x_input_data_proc.shape[1])\n",
    "\n",
    "\n",
    "\n",
    "stop\n",
    "\n",
    "### 001 load backcup parquet file \n",
    "\n",
    "#x_input_data_proc = pd.read_parquet(\"x_input_data_proc.parquet\", engine=\"pyarrow\")\n",
    "\n",
    "#x_input_data_scaled = pd.read_parquet(\"x_input_data_scaled.parquet\", engine=\"pyarrow\")\n",
    "\n",
    "### --------------------------------------------------------------------------------------------------\n",
    "\n",
    "### 002 varaible 1 splitting\n",
    "\n",
    "print(\"step 1 stablish teh varyable y to forecast, also add the speicficaiton that is wanted\")\n",
    "print(\"drop column names similar to y orioangal name\")\n",
    "vb_number=\"001\"\n",
    "variable_y= \"paye_wholesale_sa_diff1\"\n",
    "original_y=\"paye_wholesale_sa\"\n",
    "\n",
    "print(\"step 2 split data and have x dataset adn y dataset\")\n",
    "processed_data = preprocess_and_split_data(x_input_data_proc, \n",
    "                                           variable_y=variable_y,\n",
    "                                           original_y=original_y,\n",
    "                                           date_beg=\"2013-01-01\")\n",
    "\n",
    "#processed_data = preprocess_and_split_data(x_input_data_proc, variable_y=variable_y)\n",
    "x_train = processed_data[\"x_train\"]\n",
    "x_test = processed_data[\"x_test\"]\n",
    "y_train = processed_data[\"y_train\"]\n",
    "y_test = processed_data[\"y_test\"]\n",
    "y_data = processed_data[\"y_data\"]\n",
    "print(\"\")\n",
    "#x_train.shape[0]=y_train.shape[0]\n",
    "\n",
    "######################################\n",
    "print(\"y_data\")\n",
    "print(y_data.index.min())\n",
    "print(y_data.index.max())\n",
    "print(y_data.shape[0])\n",
    "print(y_data.shape[1])\n",
    "print(\"\")\n",
    "#######################################\n",
    "print(\"y_train\")\n",
    "print(y_train.index.min())\n",
    "print(y_train.index.max())\n",
    "print(y_train.shape[0])\n",
    "print(y_train.shape[1])\n",
    "print(\"\")\n",
    "#######################################\n",
    "print(\"y_test\")\n",
    "print(y_test.index.min())\n",
    "print(y_test.index.max())\n",
    "print(y_test.shape[0])\n",
    "print(y_test.shape[1])\n",
    "print(\"\")\n",
    "#######################################\n",
    "\n",
    "print(\"x_train\")\n",
    "print(x_train.index.min())\n",
    "print(x_train.index.max())\n",
    "print(x_train.shape[0])\n",
    "print(x_train.shape[1])\n",
    "print(\"\")\n",
    "#######################################\n",
    "print(\"x_test\")\n",
    "print(x_test.index.min())\n",
    "print(x_test.index.max())\n",
    "print(x_test.shape[0])\n",
    "print(x_test.shape[1])\n",
    "print(\"\")\n",
    "######################################\n",
    "print(\"Complete\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "###  002 varaible 1 algos\n",
    "\n",
    "print(\"Starting causality loop\")\n",
    "# Assuming x_train and y_train are already defined\n",
    "analysis_results_1 = causality_analysis_function_step1(x_train, \n",
    "                                                       y_train,\n",
    "    rf_n_estimators=rf_n_estimators0, \n",
    "    xgb_n_estimators=xgb_n_estimators0, \n",
    "    lgb_n_estimators=lgb_n_estimators0,\n",
    "    random_state=random_state0,\n",
    "    top_n=top_n0, \n",
    "    size_width=size_widht0,\n",
    "    size_height=size_height0, \n",
    "    vb_number=vb_number\n",
    ")\n",
    "print(\"Complete\")\n",
    "\n",
    "\n",
    "\n",
    "# Assuming x_train and y_train are defined\n",
    "# Using the function\n",
    "analysis_results_2 = causality_analysis_function_step2(y_train, \n",
    "                                                       x_train,\n",
    "                                                       max_lag=10, \n",
    "                                                       top_n=top_n0, \n",
    "                                                       size_width=size_widht0,\n",
    "                                                       size_height=size_height0, \n",
    "                                                       vb_number=vb_number)\n",
    "print(\"Complete\")\n",
    "# Using the function\n",
    "analysis_results_3 = causality_analysis_function_step3(y_train, \n",
    "                                                       x_train,\n",
    "                                                       max_lag=10, \n",
    "                                                       top_n=top_n0, \n",
    "                                                       size_width=size_widht0,\n",
    "                                                       size_height=size_height0, \n",
    "                                                       vb_number=vb_number)\n",
    "print(\"Complete\")\n",
    "#def causality_analysis_function_step4(y_train, x_train, max_iter=10, size_width=10, size_height=6, vb_number=1):\n",
    "# Using the function\n",
    "analysis_results_4 = causality_analysis_function_step4(y_train, \n",
    "                                                       x_train,\n",
    "                                                       max_iter=10, \n",
    "                                                       #top_n=top_n0, \n",
    "                                                       size_width=size_widht0,\n",
    "                                                       size_height=size_height0, \n",
    "                                                       vb_number=vb_number)\n",
    "print(\"Complete\")\n",
    "\n",
    "# Get all variables and filter by type DataFrame\n",
    "dfs = {var: obj for var, obj in globals().items() if isinstance(obj, pd.DataFrame)}\n",
    "\n",
    "# Print names of all DataFrames\n",
    "for name, df in dfs.items():\n",
    "    print(f\"DataFrame: {name} - Shape: {df.shape}\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#print(x_train[\"employment_awe_private_pay_diff1_fmt_yy_mavg24\"])\n",
    "\n",
    "\n",
    "\n",
    "###  002  varaible 1 graphs\n",
    "\n",
    "del x_train\n",
    "del x_test\n",
    "del y_train \n",
    "del y_test\n",
    "del y_data\n",
    "\n",
    "print(variable_y)\n",
    "#real_monthly_gdp_rolling_avg3_diff1_fmt_mm\n",
    "#real_service_industry_rolling_avg6_diff_fmt_qq\n",
    "#uk_wholesale_retail_sa_fmt_yy_lag1\n",
    "\n",
    "#df_example.set_index(\"date\", inplace=True)  # Ensure index is a datetime\n",
    "\n",
    "# Call the function\n",
    "plot_time_series_with_correlation(\n",
    "    df=x_input_data_proc,\n",
    "    tail_size=120,\n",
    "    variable_y=variable_y,\n",
    "    select_x_variable=\"uk_wholesale_retail_sa_fmt_yy_lag1\",\n",
    "    title_r=f\" Y ({variable_y})vs X (real_monthly_gdp)\",\n",
    "    apply_diff_y=False,   # Apply difference transformation\n",
    "    apply_shift_y=False,  # Shift X forward by 1 period\n",
    "    apply_diff_x=False,   # Apply difference transformation\n",
    "    apply_shift_x=False,  # Shift X forward by 1 period\n",
    "    color_y=\"black\",\n",
    "    color_x=\"red\",\n",
    "    output_filename=\"hv_tb_cuk_001_001_m_g_v_001.png\"\n",
    ")\n",
    "\n",
    "\n",
    "#df_example.set_index(\"date\", inplace=True)  # Ensure index is a datetime\n",
    "\n",
    "# Call the function\n",
    "plot_time_series_with_correlation(\n",
    "    df=x_input_data_proc,\n",
    "    tail_size=120,\n",
    "    variable_y=variable_y,\n",
    "    select_x_variable=\"cycle_lloyds_business_barometer_vs_3m_nsa_rolling_avg2_diff1_fmt_mm_lag1\",\n",
    "    title_r=f\" Y ({variable_y})vs X (cycle_lloyds_bss_confidence)\",\n",
    "    apply_diff_y=False,   # Apply difference transformation\n",
    "    apply_shift_y=False,  # Shift X forward by 1 period\n",
    "    apply_diff_x=False,   # Apply difference transformation\n",
    "    apply_shift_x=False,  # Shift X forward by 1 period\n",
    "    color_y=\"black\",\n",
    "    color_x=\"red\",\n",
    "    output_filename=\"hv_tb_cuk_001_001_m_g_v_002.png\"\n",
    ")\n",
    "\n",
    "\n",
    "\n",
    "# Call the function\n",
    "plot_time_series_with_correlation(\n",
    "    df=x_input_data_proc,\n",
    "    tail_size=90,\n",
    "    variable_y=variable_y,\n",
    "    select_x_variable=\"employment_adzuna_job_advertisements\",\n",
    "    title_r=f\" Y ({variable_y})vs X (employment_adzuna_job_advertisements)\",\n",
    "    apply_diff_y=True,   # Apply difference transformation\n",
    "    apply_shift_y=False,  # Shift X forward by 1 period\n",
    "    apply_diff_x=True,   # Apply difference transformation\n",
    "    apply_shift_x=True,  # Shift X forward by 1 period\n",
    "    color_y=\"black\",\n",
    "    color_x=\"red\",\n",
    "    output_filename=\"hv_tb_cuk_001_001_m_g_v_003.png\"\n",
    ")\n",
    "\n",
    "\n",
    "# Call the function\n",
    "plot_time_series_with_correlation(\n",
    "    df=x_input_data_proc,\n",
    "    tail_size=30,\n",
    "    variable_y=variable_y,\n",
    "    select_x_variable=\"employment_adzuna_job_advertisements\",\n",
    "    title_r=f\" Y ({variable_y})vs X (employment_adzuna_job_advertisements)\",\n",
    "    apply_diff_y=True,   # Apply difference transformation\n",
    "    apply_shift_y=False,  # Shift X forward by 1 period\n",
    "    apply_diff_x=True,   # Apply difference transformation\n",
    "    apply_shift_x=True,  # Shift X forward by 1 period\n",
    "    color_y=\"black\",\n",
    "    color_x=\"red\",\n",
    "    output_filename=\"hv_tb_cuk_001_001_m_g_v_004.png\"\n",
    ")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "### cleaning \n",
    "\n",
    "import gc\n",
    "import sys\n",
    "\n",
    "def cleanup_environment():\n",
    "    # List all variables in the global scope\n",
    "    globals_copy = globals().copy()\n",
    "    \n",
    "    # Delete DataFrames specifically (and optionally other variables)\n",
    "    for var_name, var_value in globals_copy.items():\n",
    "        if isinstance(var_value, pd.DataFrame):  # Checks if it's a DataFrame\n",
    "            print(f\"Deleting DataFrame: {var_name}\")\n",
    "            del globals()[var_name]\n",
    "    \n",
    "    # Trigger garbage collection\n",
    "    gc.collect()\n",
    "    print(\"Environment cleaned!\")\n",
    "\n",
    "# Call the cleanup function\n",
    "cleanup_environment()\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
